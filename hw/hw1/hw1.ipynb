{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 1 (96 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important information\n",
    "1. We provide signatures of the functions that you have to implement. Make sure you follow the signatures defined, otherwise your coding solutions will not be graded.\n",
    "\n",
    "2. Please submit the single Jupyter Notebook file, where only Python and Markdown/$\\LaTeX$ are used. Any hand-written solutions inserted by photos or in any other way **are prohibitive and will not be graded**. If you will have any questions about using Markdown, ask them!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 (Theoretical tasks)  (36 pts)\n",
    "\n",
    "_1._\n",
    "- (1 pts) What are the constants $C_1$ and $C_2$ such that $C_1 \\|x\\|_{1} \\leq \\|x\\|_{\\infty} \\leq C_2 \\| x\\|_{1}$ for any vector $x$\n",
    "- (5 pts) Prove that $\\| U A \\|_F = \\| A U \\|_F = \\| A \\|_F$ for any unitary matrix $U$.\n",
    "- (5 pts) Prove that $\\| U A \\|_2 = \\| A U \\|_2 = \\| A \\|_2$ for any unitary matrix $U$.\n",
    "  \n",
    "_2._\n",
    "- (5 pts) Using the results from the previous subproblem, prove that $\\| A \\|_F \\le \\sqrt{\\mathrm{rank}(A)} \\| A \\|_2$. _Hint:_ SVD will help you.\n",
    "- (5 pts) Show that for any $m, n$ and $k \\le \\min(m, n)$ there exists $A \\in \\mathbb{R}^{m \\times n}: \\mathrm{rank}(A) = k$, such that $\\| A \\|_F = \\sqrt{\\mathrm{rank}(A)} \\| A \\|_2$. In other words, show that the previous inequality is not strict.\n",
    "- (5 pts) Prove that if $A \\in \\mathbb{R}^{n \\times n}$ - diagonalizable matrix, then $\\det(\\exp(A)) = \\exp(\\mathrm{trace}(A))$. \n",
    "\n",
    "_Hint:_ $\\exp(S^{-1}AS) = \\sum\\limits_{k=0}^{\\infty}\\frac{(S^{-1}AS)^k}{k!} = S^{-1}\\exp(A)S$, moreover $\\exp(D) = \\mathrm{diag}(\\exp(d_1), \\exp(d_2), ..., \\exp(d_n))$ where $D = \\mathrm{diag}(d_1, d_2, ..., d_n)$ - diagonal matrix with diagonal values $d_1, d_2, ..., d_n$\n",
    "\n",
    "\n",
    "- (5 pts) Prove that $\\| A B \\|_2 \\le \\| A \\|_2 \\| B \\|_F$.\n",
    "\n",
    "_3._\n",
    "\n",
    "- (2 pts) Let $U \\in \\mathbb{C}^{n \\times k}, k < n$ be a matrix so that $U^*U = I_k$. \n",
    "Find a pseudoinverse of $UU^*$\n",
    "- (3 pts) Compute  pseudoinverse of the matrix analytically \n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "2\\\\\n",
    "3\\\\\n",
    "\\vdots\\\\\n",
    "\\sqrt{5n-1}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "-1 & 1 & -1 & \\ldots & (-1)^{n}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Problem 1 solution is here\n",
    "## Problem 1.1 solution is here\n",
    "***\n",
    "- (1 pts) What are the constants $C_1$ and $C_2$ such that $C_1 \\|x\\|_{1} \\leq \\|x\\|_{\\infty} \\leq C_2 \\| x\\|_{1}$ for any vector $x$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __SOLUTION__:\n",
    " \\\n",
    "    So we've got a squeeze theorem. Theoretically, C1 and C2 may be any Real number. But: \\\n",
    "    $ \\Vert x \\Vert_{\\infty} = \\max\\limits_i | x_i| $\\\n",
    "    $ \\Vert x \\Vert_1 = \\sum\\limits_{i=1}^{n}{|x_i|} $\\\n",
    "    Note that if any norm is 0, any other is also zero, as the vector itself is zero. \n",
    "    Also note that the norm is a strictly positive value.\\\n",
    "    Now we have an idea about the constants:\\\n",
    "    1. They are not zero\\\n",
    "    2. They are positive.\\\n",
    "    Note that $\\Vert x \\Vert_{\\infty} \\le \\Vert x \\Vert_1$ By definitions of these norms. $\\Rightarrow C_2 \\ge \\frac{\\Vert x \\Vert_{\\infty}}{\\Vert x \\Vert_{1}}$. Similarly, $C_1 \\le \\frac{\\Vert x \\Vert_{\\infty}}{\\Vert x \\Vert_{1}}$\\\n",
    "    Now let's represent the inequality with proper norm forms:\n",
    "    $$C_1 \\sum\\limits_{i=1}^{n}{|x_i|} \\leq \\max_i | x_i|\\leq C_2\\sum\\limits_{i=1}^{n}{|x_i|} $$\\\n",
    "    Note that $\\Vert x \\Vert_1 = \\sum\\limits_{i=1}^{n}|x_i| = \\sum\\limits_{i=1}^{n}|x_i|\\cdot 1 \\stackrel{\\mathrm{Cauchy-Schwartz}}{\\le} \\big(\\sum\\limits_{i=1}^{n}|x_i|^2\\big)^{0.5} \\big( \\sum\\limits_{i=1}^{n}1^2 \\big)^{0.5} = \\sqrt{n} \\Vert x \\Vert_2$, i.e.\\\n",
    "    $\\Vert x \\Vert_1 \\le \\sqrt{n} \\Vert x \\Vert_2$. We've set the upper bound for the L1 norm with an L2 norm.\\\n",
    "    Note that \\\n",
    "    $\\Vert x \\Vert_2 = (\\Vert x \\Vert_1)^2 - x^Tx$\\\n",
    "    $x^Tx = \\Vert x \\Vert_1^2 - \\Vert x \\Vert_2 \\le \\Vert x \\Vert_2^2$\n",
    "\\\n",
    "   no more ideas. but in general we need to bound the L2 with L_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "a = sorted([1*random.random() for i in range(10)]) # arbitrary vector\n",
    "\n",
    "chebyshev = np.linalg.norm(a, ord=2)\n",
    "manhattan = np.linalg.norm(a, ord=1)\n",
    "\n",
    "c = chebyshev/manhattan\n",
    "for i in range(a.__len__()):\n",
    "    print(\"{:.3f}\".format(a[i]))\n",
    "print(\"\\n\")\n",
    "print(\"{}*{:.3f} <= {:.3f} <= {}*{:.3f}\".format(\"c1\", manhattan, chebyshev, \"c2\", manhattan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "- (5 pts) Prove that $\\| U A \\|_F = \\| A U \\|_F = \\| A \\|_F$ for any unitary matrix $U$.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __SOLUTION__:\n",
    " \\\n",
    "Mind refreshers: \n",
    "- For a rectangular unitary matrices of size $m\\times n$ ($n\\not= m$) only one of the equalities can hold\\\n",
    "    - $ U^HU = I_n$ –– left unitary for $m>n$\n",
    "    - $ UU^H = I_m$  –– right unitary for $m<n$\n",
    "- $AA^T$,$A^TA$, $A^HA$ and $AA^H$ are square matrices\n",
    "- if $AB$ is a square matrix, $ tr(AB) = tr(BA)$\\\n",
    "    $tr(AB) = {\\sum_i(AB)_{ii}} = {\\sum_{i}\\sum_{j}}a_{ij}b_{ji} = {\\sum_{j}\\sum_{i}}b_{ji}a_{ij} = {\\sum_i(AB)_{jj}} = tr(BA)$\\\n",
    "    \\\n",
    "    \\\n",
    "    Consider $\\Vert A \\Vert_F^2$\\\n",
    "    **Lemma 1**: $\\Vert A \\Vert_F = \\sqrt{(A,A)_F}$\\\n",
    "    By definition, \n",
    "    $ \\Vert A \\Vert_F = \\sqrt{\\sum_{i=1}^{n} \\sum_{j=1}^{m} |a_{ij}|^2}$\\\n",
    "    $$ (A, A)_F = {\\sum_{i=1}^{n}\\sum_{j=1}^{m}} \\overline{a}_{ij} a_{ij} = {\\sum_{i=1}^{n}\\sum_{j=1}^{m} |a_{ij}|^2}$$\\\n",
    "    One can see that lemma holds true. Lemma 1 QED. \n",
    "    \\\n",
    "    **Lemma 2**: $(A,A)_F = tr(A^H A)$\\\n",
    "    $\\begin{aligned}\n",
    "(A,A)_F=\\sum_{i, j} \\overline{a_{i j}} a_{i j} = \n",
    "& \\bar{a}_{11} a_{11}+\\bar{a}_{12} a_{12}+\\cdots+\\bar{a}_{1 m} a_{1 m} \\\\\n",
    "&+\\bar{a}_{21} a_{21}+\\bar{a}_{22} a_{22}+\\cdots+\\bar{a}_{2 m} a_{2 m} \\\\\n",
    "& \\vdots \\\\\n",
    "&+\\bar{a}_{n 1} a_{n 1}+\\bar{a}_{n 2} a_{n 2}+\\cdots+\\bar{a}_{n m} a_{n m}\n",
    "\\end{aligned}$\\\n",
    "    Which is a sum of diagonal elements of $A^H A$, which is a trace of $A^H A$ by definition. Lemma 2 QED.    \n",
    "    \\\n",
    "    Using lemmas 1 and 2, $\\Vert A \\Vert_F^2 = tr(A^H A)$\\\n",
    "    \\\n",
    "    Now, \\\n",
    "    $\\Vert UA \\Vert_F^2 = tr((UA)^H (UA)) = tr(A^HU^H U A) = tr(A^H I A) = tr(A^H A) = \\Vert A \\Vert_F^2 \\Rightarrow \\Vert UA \\Vert_{F} = \\Vert A \\Vert_{F}$\\\n",
    "    Analogously,\n",
    "    $\\Vert AU \\Vert_F^2 = tr((AU)(AU)^H) = tr(A U U^H A^H) = tr(A I A^H) = tr(A^H A) = \\Vert A \\Vert_F^2 \\Rightarrow \\Vert AU \\Vert_{F} = \\Vert A \\Vert_{F}$\\\n",
    "    \\\n",
    "    So, $\\| U A \\|_F = \\| A U \\|_F = \\| A \\|_F \\forall$ unitary matrix $U$ QED. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "- (5 pts) Prove that $\\| U A \\|_2 = \\| A U \\|_2 = \\| A \\|_2$ for any unitary matrix $U$.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __SOLUTION__:\n",
    "\\\n",
    "Since the 2-norm is submultiplicative, and since the product of norms is commutative, \\\n",
    "$\\Vert UA \\Vert_2 \\le \\Vert U \\Vert_2 \\Vert A  \\Vert_2 \\le \\sigma_{\\max}(U)\\sqrt{\\lambda_{max}(A^H A)}$\\\n",
    "$\\Vert AU \\Vert_2 \\le \\Vert A \\Vert_2 \\Vert U  \\Vert_2 \\le \\sigma_{\\max}(U)\\sqrt{\\lambda_{max}(A^H A)}$\\\n",
    "So at least the upper bounds are the same.\\\n",
    "As long as x =/= 0,\\\n",
    "$\\Vert U A \\Vert_{2}=\\sup \\frac{\\Vert U A x \\Vert_{2}}{\\Vert x \\Vert_{2}} = \\sup \\frac{\\sqrt{(U A x, U A x)}}{\\Vert x \\Vert_{2}}=$\\\n",
    "!!! now we assume that the L2 norm of x should be ==1 to maximize the supremum of matrix' 2-norm!!!\\\n",
    "$=\\sup \\sqrt{\\left(U^H U A x, A x\\right)}=\\sup \\sqrt{(A x, A x)}=\\Vert A\\Vert_{2}$\n",
    "\\\n",
    "For $\\Vert A U \\Vert _{2} =\\sup \\frac{\\Vert A U x \\Vert_{2}}{\\Vert x \\Vert_{2}}$, introduce $y=Ux$, $x=U^Hy$. Recall that for vector L-2 norm, ||Ux|| = ||x|| as unitary matrices perserve the vector L2 norm.\\\n",
    "$\\Vert A U \\Vert _{2} =\\sup \\frac{\\Vert A y \\Vert_{2}}{\\Vert U^Hy \\Vert_{2}}=\\sup \\frac{\\Vert A y \\Vert_{2}}{\\Vert y \\Vert_{2}}$. We do the same logic as we did with $\\Vert U A \\Vert$ and we indeed prove the statement.\n",
    "QED.\n",
    "\n",
    "Problem 1.1 done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Problem 1.2 solution is here\n",
    "***\n",
    "- (5 pts) Using the results from the previous subproblem, prove that $\\| A \\|_F \\le \\sqrt{\\mathrm{rank}(A)} \\| A \\|_2$. _Hint:_ SVD will help you.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SOLUTION__:\n",
    "\\\n",
    "We know that $\\Vert A \\Vert_F = \\sqrt{tr(A^HA)}$ from previous subproblem.\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do an SVD, keeping in mind that $U$ and $V$ are unitary matrices:\\\n",
    "![](https://i.stack.imgur.com/g9dJT.png)\\\n",
    "![](https://i.stack.imgur.com/s6yX1.png)\\\n",
    "![](https://i.stack.imgur.com/Pwwfu.png)\\\n",
    "(good SVD visualizations, taken from [here](https://math.stackexchange.com/questions/1087064/non-zero-eigenvalues-of-aat-and-ata)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A = U\\Sigma V^H$;; $A^H = (U\\Sigma V^H)^H = {V^H}^H(U \\Sigma)^H = V \\Sigma^H U^H $, \\\n",
    "$tr(A^HA) = tr(V \\Sigma^H U^HU \\Sigma V^H) = tr(V \\Sigma^H \\Sigma V^H)$\\\n",
    "Note that because of $\\Sigma$ being a diagonal matrix, $\\Sigma^H \\Sigma = \\Sigma \\Sigma^H = diag(\\sigma_1^2,\\dots,\\sigma_r^2)$. Keep in mind that $\\sigma_1^2 \\ge \\dots \\ge \\sigma_r^2$, and any $\\sigma_i = 0$ for $i>r$, where $r = rank(A)$. \\\n",
    "Let us call $\\Sigma \\Sigma^H = \\Sigma^H \\Sigma = S$ $\\Rightarrow $\\\n",
    "$(1)~~~~tr(A^HA) = tr(V \\Sigma^H \\Sigma V^H) = tr(V S V^H) = tr(S) = \\sigma_1^2 + \\dots + \\sigma_r^2$ \n",
    "\\\n",
    "\\\n",
    "Now let's source the definition of the spectral norm as an induced one: \\\n",
    "$\\Vert A \\Vert_2 = \\sup\\limits_{x\\ne 0} \\frac{\\Vert Ax \\Vert}{\\Vert x \\Vert_2} = \\sup\\limits_{x\\ne 0} \\frac{\\sqrt{(Ax,Ax)}}{\\Vert x \\Vert_2}=$\\\n",
    "!!! now we assume that the L2 norm of x should be ==1 to maximize the supremum of matrix' 2-norm!!!\\\n",
    "$= \\sup\\limits_{||x||_2 = 1}\\sqrt{(U\\Sigma V^Hx, U\\Sigma V^H x)} = \\sup\\limits_{||x||_2 = 1}\\sqrt{(Sx,x)}$\\\n",
    "\\\n",
    "While calculating the scalar product $(U\\Sigma V^Hx, U\\Sigma V^H x)$, we keep in mind that $U$ and $V$ are unitary matrices. This means that every time $v^H v$ is either 0 or 1. So, $(U\\Sigma V^Hx, U\\Sigma V^H x) = (\\Sigma x, \\Sigma x) = (Sx,x) = \\sum\\limits_{i=1}^r \\sigma_i^2\\cdot x_i^2$.\\\n",
    "\\\n",
    "Since we set $\\Vert x \\Vert_2 = 1 \\Rightarrow (x,x) = \\Vert x \\Vert_2^2 = 1$. Thus, $(Sx,x) = \\sum\\limits_{i=1}^r \\sigma_i^2 = tr(S)$\\\n",
    "The supremum is reachible if every singular value is the largest, so $\\sigma_1^2 = \\dots = \\sigma_r^2 \\Rightarrow tr(S) = \\sum\\limits_{i=1}^r \\sigma_1^2 = r \\cdot \\sigma_1^2$\\\n",
    "$(2)~~~~\\Vert A \\Vert_2 =  \\sup\\limits_{||x||_2 = 1}\\sqrt{(Sx,x)} = \\sqrt{r \\cdot \\sigma_1^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using (1), (2) and $\\Vert A \\Vert_F = \\sqrt{tr(A^HA)}$:\\\n",
    "$\\begin{cases}\n",
    "\\Vert A \\Vert_F = \\sqrt{tr(A^HA)}\\\\\n",
    "\\mathrm{tr}(A^HA) = \\sum\\limits_{i=1}^{\\mathrm{rank}(A)}\\sigma_i^2 \\\\\n",
    "\\Vert A \\Vert_2 = \\sigma_1 \\sqrt{\\mathrm{rank}(A)}\\\\\n",
    "\\end{cases}$\\\n",
    "// reminder:  $\\sigma_1^2 \\ge \\dots \\ge \\sigma_r^2$\n",
    "\\\n",
    "Given all the listed statements, it becoes obvious that $\\| A \\|_F \\le \\sqrt{\\mathrm{rank}(A)} \\| A \\|_2$ is true QED."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "- (5 pts) Show that for any $m, n$ and $k \\le \\min(m, n)$ there exists $A \\in \\mathbb{R}^{m \\times n}: \\mathrm{rank}(A) = k$, such that $\\| A \\|_F = \\sqrt{\\mathrm{rank}(A)} \\| A \\|_2$. In other words, show that the previous inequality is not strict.\n",
    "***\n",
    "__SOLUTION__:\n",
    "\\\n",
    "In general, $\\sigma_1^2 \\ge \\dots \\ge \\sigma_k^2$\n",
    "\\\n",
    "From previous subtask,\\\n",
    "$\\begin{cases}\n",
    "\\Vert A \\Vert_F = \\sqrt{tr(A^HA)}\\\\\n",
    "\\mathrm{tr}(A^HA) = \\sum\\limits_{i=1}^{\\mathrm{rank}(A)}\\sigma_i^2 \\\\\n",
    "\\Vert A \\Vert_2 = \\sigma_1 \\sqrt{\\mathrm{rank}(A)}\\\\\n",
    "\\end{cases}$\n",
    "\n",
    "so, $\\| A \\|_F = \\sqrt{\\mathrm{rank}(A)} \\| A \\|_2$. Substituting:\\\n",
    "$\\sqrt{\\sum\\sigma_i^2} =  \\sqrt{\\mathrm{rank}(A)} \\sigma_1 \\sqrt{\\mathrm{rank}(A)} = \\sigma_1 \\cdot\\mathrm{rank}(A)$\\\n",
    "$\\sqrt{\\sum_i^k\\sigma_i^2} =  \\sigma_1 \\cdot\\mathrm{rank}(A) = \\sqrt{\\sum_i^k\\sigma_1^2}$\\\n",
    "$\\sum_i^k\\sigma_i^2 = \\sum_i^k\\sigma_1^2 \\Rightarrow 1 + \\frac{\\sigma_2^2 + ... + \\sigma_r^2}{\\sigma_1^2} = k$\n",
    "\n",
    "This is only possible if $\\sigma_1=\\sigma_2=\\dots=\\sigma_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "- (5 pts) Prove that if $A \\in \\mathbb{R}^{n \\times n}$ - diagonalizable matrix, then $\\det(\\exp(A)) = \\exp(\\mathrm{trace}(A))$. \n",
    "\n",
    "<details>\n",
    "<summary><i>Hint:</i></summary>\n",
    "<br>\n",
    "$\\exp(S^{-1}AS) = \\sum\\limits_{k=0}^{\\infty}\\frac{(S^{-1}AS)^k}{k!} = S^{-1}\\exp(A)S$, moreover $\\exp(D) = \\mathrm{diag}(\\exp(d_1), \\exp(d_2), ..., \\exp(d_n))$ where $D = \\mathrm{diag}(d_1, d_2, ..., d_n)$ - diagonal matrix with diagonal values $d_1, d_2, ..., d_n$\n",
    "</details>\n",
    "\n",
    "***\n",
    "__SOLUTION__:\n",
    "\\\n",
    "minus 5 points from Gryffindor, asuming CDISE is Gryffindor.\\\n",
    "I provide no solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "- (5 pts) Prove that $\\Vert A B \\Vert_2 \\le \\Vert A \\Vert_2 \\Vert B \\Vert_F$.\n",
    "***\n",
    "__SOLUTION__:\n",
    "\\\n",
    "Recall, that $\\Vert A \\Vert_2 = \\sigma_{11}~~~~| i=j = 1: \\sigma_{11} = \\max({\\sigma_{ij}})$\\\n",
    "Let's do an SVD once more:\\\n",
    "$ A = U \\Sigma V^H$\\\n",
    "$\\Vert U\\Sigma V^H B\\Vert_F \\le \\Vert A\\Vert_2 \\Vert B\\Vert_F$\\\n",
    "Recall that unitary matrices preserve both the spectral and frobenius norms, so we can multiply $B$ by $V^*$:\\\n",
    "$\\Vert U \\Sigma V^HB\\Vert_F \\le \\Vert A\\Vert_2 \\Vert V^HB\\Vert_F$\\\n",
    "Note, that $\\Vert U \\Sigma V^HB\\Vert_F =   \\sqrt{\\sum_{i,j}^k | \\sigma_{ij} (V^HB)_{ij} |^2}$\\\n",
    "$\\Vert V^HB \\Vert_F = \\sqrt{\\sum_{i,j}^k |  (V^HB)_{ij} |^2}$\n",
    "\n",
    "And that means, that we can take out sigma: $\\Vert \\Sigma (V^HB) \\Vert_F \\le \\sqrt{\\sigma_{11}^2 \\cdot \\sum_{i,j}^k |(V^HB)_{ij} |^2} =  \\sigma_{11} \\sqrt{\\sum_{i,j} | V^*B |^2} \\to \\Vert A \\Vert_2 \\cdot \\Vert B\\Vert_F$\\\n",
    "We see that the inequality holds true QED."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Problem 1.3 solution is here\n",
    "***\n",
    "- (2 pts) Let $U \\in \\mathbb{C}^{n \\times k}, k < n$ be a matrix so that $U^*U = I_k$. \n",
    "Find a pseudoinverse of $UU^*$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SOLUTION__:\n",
    "\\\n",
    "First, \n",
    "$A^{\\dagger} = \\lim\\limits_{\\alpha \\rightarrow 0}(\\alpha I + A^H A)^{-1} A^H$ is called pseudoinverse of the matrix $A$ by definition.\\\n",
    "\\\n",
    "$(UU^H)^\\dagger = \\lim\\limits_{\\alpha \\rightarrow 0}(\\alpha I + (UU^H)^H UU^H)^{-1} (UU^H)^H = $\n",
    "$\\lim\\limits_{\\alpha \\rightarrow 0}(\\alpha I + UU^H UU^H)^{-1} UU^H = $\n",
    "$\\lim\\limits_{\\alpha \\rightarrow 0}(\\alpha I + UU^H)^{-1}UU^H$\n",
    "\\\n",
    "As $\\alpha \\rightarrow 0$, we get $(\\alpha I + (UU^H)^H UU^H)^{-1} \\rightarrow (UU^H)^{-1} UU^H$, which means that \\\n",
    "$(UU^H)^\\dagger = (UU^H)^{-1} UU^H$. \\\n",
    "Given that $UU^H$ is of size ${n\\times n}$\\\n",
    "$(UU^H)^{-1} UU^H = I_n$ by definition of an inverse matrix.\\\n",
    "So the pseudoinverse of $UU^H$ is an indentity matrix of size ${n\\times n}$. \n",
    "\n",
    "Matrix $A^\\dagger\\in \\mathbb{C}^{k \\times n}$ is pseudoinverse to $A \\in \\mathbb{C}^{n \\times k}$ iff (Moore-Penrose criterions):\n",
    "\\begin{cases}\n",
    "AA^\\dagger A = A\\\\\n",
    "A^\\dagger AA^\\dagger = A^\\dagger\\\\\n",
    "(AA^\\dagger)^H = AA^\\dagger \\\\\n",
    "(A^\\dagger A)^H = A^\\dagger A\n",
    "\\end{cases}\n",
    "\n",
    "Let's check if the identity matrix is indeed a pseudoinverse of $UU^H$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) \n",
    "$UU^H = U I_k U^H = UU^H UU^H$\\\n",
    "Nothe, that $U U^HU U^H = U U^H I_n U U^H$.\\\n",
    "Rename $UU^H \\to X$.\\\n",
    "$ UU^H = U U^H I_n U U^H$\\\n",
    "$ X~~~~~~= X I_n X$\\\n",
    "Note, that if $I_n = X^\\dagger$, condition (1) of definition stands true. \n",
    "\n",
    "2) \n",
    "Consider $I_n X I_n$. It should be equal to $I_n$. Recall that $X = X X$ which makes sense if and only if $X = I_n$ or zeros, but everything is zero if $X = 0$.\n",
    "Condition (2) standes true. \n",
    "\n",
    "3) \n",
    "$(XI_n)^H = (UU^H I_n)^H = I_n^H (UU^H)^H = I_n {U^H}^H U^H = I_n U U^H = UU^H I_n= XI_n$. Condition (3) holds true. \n",
    "\n",
    "4) \n",
    "Basically the same as in step 3. $X$ and $I_n$ are both square matrices, so we can mulpiply with no affect both on the left and on the right. \n",
    "\n",
    "QED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "- (3 pts) Compute  pseudoinverse of the matrix analytically \n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "2\\\\\n",
    "3\\\\\n",
    "\\vdots\\\\\n",
    "\\sqrt{5n-1}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "-1 & 1 & -1 & \\ldots & (-1)^{n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SOLUTION__:\n",
    "\\\n",
    "Let's do SVD:\n",
    "$$A = U \\Sigma V^*,$$\n",
    "$$A^{\\dagger} = V \\Sigma^{\\dagger} U^*,$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A = \\begin{bmatrix}\n",
    "-2 & 2 & -2 & \\dots\\\\\n",
    "-3 & 3 & -3 & \\dots\\\\\n",
    "-\\sqrt{14} & \\sqrt{14} & -\\sqrt{14} & \\dots\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{bmatrix}$\n",
    "$~~~~~~A^H= A^T = \\begin{bmatrix}\n",
    "-2 & -3 & -\\sqrt{14} & \\dots\\\\\n",
    "~~2 &~~3&~~\\sqrt{14} & \\dots\\\\\n",
    "-2 & -3 & -\\sqrt{14} & \\dots\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{bmatrix}$\\\n",
    "Columns of $A$ are lin.dep. and repetative, which means that $\\mathrm{rank}A = 1$ (given the equality of column and row ranks, same holds for $A^H$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider $AA^H =\\begin{bmatrix}\n",
    "4n     &\\dots  &\\dots & \\dots & 2n\\sqrt{5i-1} \\\\\n",
    "\\vdots & 9n    &\\dots & \\dots & 3n\\sqrt{5i-1} \\\\\n",
    "\\vdots & \\vdots & 14n & \\dots& n\\sqrt{14}\\sqrt{5i-1} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & n\\sqrt{19}\\sqrt{5i-1} \\\\\n",
    "&   & \\vdots & & n(5i-1)\\\\\n",
    "\\end{bmatrix}$\n",
    "\\\n",
    "\\\n",
    "$AA^H = U\\Sigma V^H (U\\Sigma V^H)^H = U\\Sigma  V^H V(U \\Sigma )^H = U \\Sigma^2 U^H$\n",
    "\\\n",
    "$\\Sigma^2 =  U^H AA^H U$\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider $A^HA = \\sum\\limits_{i=1}^n 5n-1\\begin{bmatrix}\n",
    "1 & -1 & 1 & \\dots\\\\\n",
    "-1 & 1 & -1 & \\dots\\\\\n",
    "1 & -1 & 1 & \\dots\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{bmatrix}$\n",
    "\\\n",
    "$A^HA = (U\\Sigma V^H)^H U\\Sigma V^H  = V\\Sigma^H  U^H U \\Sigma V^H = V \\Sigma^2 V^H$\n",
    "\\\n",
    "$\\Sigma^2 =  V^H AA^H V$\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma_1^2 = \\max(\\lambda_{A^HA})$\n",
    "\n",
    "let $\\kappa = (\\sum\\limits_{i=1}^n 5n-1)$\\\n",
    "As for any square matrix, $\\mathrm{tr}A = \\sum\\limits_i^na_{ii}\\lambda_i$: \n",
    "\\\n",
    "$\n",
    "\\lambda = \\mathrm{tr}(A^HA) = \\mathrm{tr}\\Big(\\kappa \\begin{bmatrix}\n",
    "1 & -1 & 1 & \\dots\\\\\n",
    "-1 & 1 & -1 & \\dots\\\\\n",
    "1 & -1 & 1 & \\dots\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{bmatrix}\\Big) = \\kappa\\cdot\\mathrm{tr}(A^HA) = \\kappa\n",
    "$\\\n",
    "\\\n",
    "$\\lambda = \\kappa$\\\n",
    "we take max eigenvalue, $\\sigma_1^2 = \\kappa = (\\sum\\limits_{i=1}^n 5n-1)$\\\n",
    "$\\Sigma = \\begin{bmatrix} \\sqrt{\\sum_{i=1}^n 5n-1} & 0 & \\dots \\\\\n",
    "0 & 0 & \\dots \\\\\n",
    "\\vdots & \\vdots & \\ddots\n",
    "\\end{bmatrix} \\Rightarrow \\Sigma^\\dagger = \\begin{bmatrix} \\frac{1}{\\sqrt{\\sum_{i=1}^n 5n-1}} & 0 & \\dots \\\\\n",
    "0 & 0 & \\dots \\\\\n",
    "\\vdots & \\vdots & \\ddots\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the property from the proof of SVD: $V^*_rA^*V_r = \\Sigma^2_r \\to (\\Sigma^{-1}_rV^*_rA^*)(AV_r \\Sigma^{-1}_r) = I$ and that $U_r = AV_r \\Sigma^{-1}_r$ satisfies $U^*_rU_r = I$\n",
    "\n",
    "$\\frac{1}{\\sigma_1^2}AA^H = u_1u_1^H$; assume that $ u_1u_1^H = UU^H $ \\\n",
    "$\\frac{1}{\\sigma_1^2}A^HA = v_1v_1^H$; assume that $ v_1v_1^H = VV^H $ \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "***\n",
    "# Problem 2 (Matrix calculus) (15 pts)\n",
    "\n",
    "_1._ (11 pts) Consider the following function\n",
    "\n",
    "$$ F(U, V) = \\frac{1}{2}\\|X - UDV\\|_F^2, $$\n",
    "\n",
    "where $X \\in \\mathbb{R}^{n \\times n}$, $U \\in \\mathbb{R}^{n \\times k}$, $V \\in \\mathbb{R}^{k \\times n}$, $k < n$ and $D = diag(d_1, \\ldots, d_k)$ is given diagonal matrix.\n",
    "\n",
    "- (2 pts) Derive analytical expression for the gradient of the function $F$ with respect to $U$ \n",
    "- (2 pts) Derive analytical expression for the gradient of the function $F$ with respect to $V$\n",
    "- (7 pts) Estimate computational complexity of computing these gradients (in big-O notation). Also, compare timing of analytical computations versus timing the automatic differentiation with JAX. Study some range of dimensions to extract asymptotic complexity and make a conclusion, what approach is faster. Plot the dependence of running time on the dimension (row and column separately) to proof your conclusion \n",
    "\n",
    "_2._ (4 pts) Derive analytical expression for the gradient and hessian of the function $f$:\n",
    "\n",
    "$$ R(x) = \\frac{(Ax, x)}{(x, x)}, $$\n",
    "\n",
    "where $A$ is a symmetric real matrix. Why the gradient of this function is important in NLA you will know in the lectures later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "# Problem 2 solution is here\n",
    "## Problem 2.1 solution is here\n",
    "***\n",
    "_1._ (11 pts) Consider the following function\n",
    "\n",
    "$$ F(U, V) = \\frac{1}{2}\\|X - UDV\\|_F^2, $$\n",
    "\n",
    "where \n",
    "$X \\in \\mathbb{R}^{n \\times n}$, $U \\in \\mathbb{R}^{n \\times k}$, $V \\in \\mathbb{R}^{k \\times n}$, $k < n$ and $D = diag(d_1, \\ldots, d_k)$ is given diagonal matrix.\n",
    "\n",
    "- (2 pts) Derive analytical expression for the gradient of the function $F$ with respect to $U$ \n",
    "- (2 pts) Derive analytical expression for the gradient of the function $F$ with respect to $V$\n",
    "- (7 pts) Estimate computational complexity of computing these gradients (in big-O notation). Also, compare timing of analytical computations versus timing the automatic differentiation with JAX. Study some range of dimensions to extract asymptotic complexity and make a conclusion, what approach is faster. Plot the dependence of running time on the dimension (row and column separately) to proof your conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__SOLUTION__ for \n",
    "\n",
    "> Derive analytical expression for the gradient of the function $F$ with respect to $U$\\\n",
    "> Derive analytical expression for the gradient of the function $F$ with respect to $V$\\\n",
    "\n",
    "`made possible with` [`the matrix cookbook`](http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf).\n",
    "\n",
    "Let $A = X - UDV$.\\\n",
    "$ F(U, V) = \\frac{1}{2}\\Vert X - UDV\\Vert _F^2 \\equiv \\frac{1}{2}\\mathrm{tr}(A^H A)=\\frac{1}{2}\\mathrm{tr}(AA^H)$ \\\n",
    "$\\frac{1}{2}\\mathrm{tr}\\big((X^T - (UDV)^T)(X - UDV)\\big) = $\\\n",
    "$\\frac{1}{2}\\Big[\\mathrm{tr}\\Big(X^TX\\Big) - \\mathrm{tr}\\Big(X^TUDV\\Big) - \\mathrm{tr}\\Big((UDV)^TX\\Big) + \\mathrm{tr}\\Big((UDV)^TUDV\\Big)\\Big]$\\\n",
    "\\\n",
    "Recall. that $\\mathrm{tr}(A^T B)=\\mathrm{tr}(B^T A)=\\mathrm{tr}(A B^T)=\\mathrm{tr}(B A^T)$\\\n",
    "\\\n",
    "$\\frac{1}{2}\\Big[\\mathrm{tr}\\Big(X^TX\\Big) - \\mathrm{tr}\\Big(X^TUDV\\Big) - \\mathrm{tr}\\Big((UDV)^TX\\Big) + \\mathrm{tr}\\Big((UDV)^TUDV\\Big)\\Big] = $\\\n",
    "\\\n",
    "\\\n",
    "$\\frac{1}{2}\\Big[\\mathrm{tr}\\Big(X^TX\\Big) - 2\\mathrm{tr}\\Big(X^TUDV\\Big) + \\mathrm{tr}\\Big((UDV)^TUDV\\Big)\\Big] = $\\\n",
    "$\\frac{1}{2}\\Big[\\mathrm{tr}\\Big(X^TX\\Big) - 2\\mathrm{tr}\\Big(X^TUDV\\Big) + \\mathrm{tr}\\Big(V^TD^TU^TUDV\\Big)\\Big]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- w.r.t U\n",
    "\n",
    "    $\\frac{1}{2}\\Big[\\mathrm{tr}\\Big(X^TX\\Big) - 2\\mathrm{tr}\\Big(X^TUDV\\Big) + \\mathrm{tr}\\Big(V^TD^TU^TUDV\\Big)\\Big]\\frac{\\partial}{\\partial U} = $\\\n",
    "    $\\frac{1}{2}\\Big[\\mathrm{tr}\\Big(X^TX\\Big) - 2\\mathrm{tr}\\Big(X^TUDV\\Big) + \\mathrm{tr}\\Big(V^TD^TU^T I UDV\\Big)\\Big]\\frac{\\partial}{\\partial U} = $\\\n",
    "    \n",
    "    $\\frac{1}{2}\\Big[0 - 2DVX^T + DV V^TD^T UI + DV V^TD^TUI\\Big] = $\\\n",
    "$ -DVX^T + DV V^TD^T U$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- w.r.t V\n",
    "\n",
    "    $\\frac{1}{2}\\Big[\\mathrm{tr}\\Big(X^TX\\Big) - 2\\mathrm{tr}\\Big(X^TUDV\\Big) + \\mathrm{tr}\\Big(V^TD^TU^TUDV\\Big)\\Big]\\frac{\\partial}{\\partial V} = $\\\n",
    "    $\\frac{1}{2}\\Big[\\mathrm{tr}\\Big(X^TX\\Big) - 2\\mathrm{tr}\\Big(X^TUDV\\Big) + \\mathrm{tr}\\Big(UDVV^TD^TU^T\\Big)\\Big]\\frac{\\partial}{\\partial V} = $\\\n",
    "    $\\frac{1}{2}\\Big[0 - 2X^TUD + V^TD^TU^TUD + V^TD^TU^TUD \\Big] = $\\\n",
    "    $ - X^TUD + V^TD^TU^TUD$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "***\n",
    "__SOLUTION__ for \n",
    ">Estimate computational complexity of computing these gradients (in big-O notation). Also, compare timing of analytical computations versus timing the automatic differentiation with JAX. Study some range of dimensions to extract asymptotic complexity and make a conclusion, what approach is faster. Plot the dependence of running time on the dimension (row and column separately) to proof your conclusion \n",
    "\n",
    "First, let's get an analyt. est. of the complexity for derivative w.r.t. both U and V. Note that there is the same amount of operations for both derived derivatives.For simplicity, we disregard applicability of Strassen algorithm and source the na:ive one.\\\n",
    "D is diagonal, so multiplying by it is not O(n^3), just O(n^2) (worst case, to \"magnitude\" every element of another matrix).\\\n",
    "O(n^2 + n^3 + 4n^3 + 4n^3 + n^2) = O(9n3).\n",
    "\n",
    "Now let's do JAX stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "from time import time\n",
    "\n",
    "@jax.jit\n",
    "def froNorm_original(X, U, D, V): \n",
    "    func = X - U @ (D @ V)\n",
    "    fronorm = 1/2*(jnp.linalg.norm(func, 'fro'))**2\n",
    "    return fronorm\n",
    "\n",
    "@jax.jit\n",
    "def analyt_dfdU(X, U,D, V):\n",
    "    res = - D @(V @ X.T) + D @ (V @ (V.T @ (D.T @ U)))\n",
    "    # -DVX^T + DV V^TD^T U\n",
    "    return res.T\n",
    "\n",
    "@jax.jit\n",
    "def analyt_dfdV(X, U,D, V):\n",
    "    res = - X.T @ (U @ D) + V.T @ (D.T @ (U.T @ (U @ D))) \n",
    "    # -X^TUD + V^TD^TU^TUD\n",
    "    return res.T\n",
    "\n",
    "jgrad_dfdU = jax.grad(froNorm_original,  argnums=1, has_aux=False)\n",
    "jgrad_dfdV = jax.grad(froNorm_original,  argnums=3, has_aux=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_51086/370625793.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRNGKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m117\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mXx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mUu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "nn = 4\n",
    "kk = 4\n",
    "key = random.PRNGKey(117)\n",
    "Xx = random.normal(key, (nn, nn)) \n",
    "Uu = jnp.eye(nn, kk) \n",
    "Vv = jnp.eye(kk, nn)\n",
    "Dd = jnp.eye(kk, kk)\n",
    "\n",
    "print('an & jx deriv close (df/dU)\\r\\n:', np.isclose(jgrad_dfdU(Xx,Uu,Dd,Vv),  analyt_dfdU(Xx,Uu,Dd,Vv)), '\\r\\n')\n",
    "print('an & jx deriv close (df/dV)\\r\\n:', np.isclose(jgrad_dfdV(Xx,Uu,Dd,Vv),  analyt_dfdV(Xx,Uu,Dd,Vv)), '\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_set = [1, 5, 10, 50, 100, 500, 600, 700, 800, 900, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "k_set = [1, 5, 10, 50, 100, 500, 600, 700, 800, 900, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "\n",
    "meanvals_jaxgrad = np.zeros((len(n_set), len(k_set)))\n",
    "meanvals_analyt = np.zeros((len(n_set), len(k_set)))\n",
    "\n",
    "for i, n in enumerate(n_set):\n",
    "    for j, k in enumerate(k_set):\n",
    "        jxt = []\n",
    "        ant = []\n",
    "\n",
    "        for z in range(1):\n",
    "            key = random.PRNGKey(np.random.randint(0, 1e16, 1)[0])\n",
    "            X = random.normal(key, (n, n)) \n",
    "            U = random.normal(key, (n, k)) \n",
    "            V = random.normal(key, (k, n))\n",
    "            D = random.normal(key, (k, k))\n",
    "\n",
    "            jx_profiler = time()\n",
    "            jaxg = jgrad_dfdV(X,U,D,V).block_until_ready()\n",
    "            jx_profiler = time() - jx_profiler\n",
    "\n",
    "            an_profiler = time()\n",
    "            anag = analyt_dfdV(X,U,D,V)\n",
    "            an_profiler = time() - an_profiler\n",
    "\n",
    "            jxt.append(jx_profiler)\n",
    "            ant.append(an_profiler)\n",
    "        meanvals_jaxgrad[i,j] = abs(np.mean(np.array(jxt)))\n",
    "        meanvals_analyt[i,j] = abs(np.mean(np.array(ant)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37195/4283115449.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRNGKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m117\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(k_set, meanvals_jaxgrad[-1, :], label='Jax autograd')\n",
    "plt.plot(k_set, meanvals_analyt[-1, :], label='analytic grad')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('k', fontsize=14)\n",
    "plt.ylabel('Mean execution time, log(ms)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_51086/518514196.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mn_compl_jx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolyfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeanvals_jaxgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mn_compl_an\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolyfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeanvals_analyt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Estimated complexity with respect to $n$ is: \\n O({:.3f}) for JAX; \\n O({:.3f} for analytical approach)'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_compl_jx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_compl_an\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "n_compl_jx = np.polyfit(np.log10(n_set), np.log10(meanvals_jaxgrad[-1,:]), 1)[0]\n",
    "n_compl_an = np.polyfit(np.log10(n_set), np.log10(meanvals_analyt[-1,:]), 1)[0]\n",
    "print('Estimated complexity with respect to $n$ is: \\n O({:.3f}) for JAX; \\n O({:.3f} for analytical approach)'.format(n_compl_jx, n_compl_an))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(k_set, meanvals_jaxgrad[:, -1], label='Jax autograd')\n",
    "plt.plot(k_set, meanvals_analyt[:, -1], label='analytic grad')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('k', fontsize=14)\n",
    "plt.ylabel('Mean execution time, log(ms)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_compl_jx = np.polyfit(np.log10(n_set), np.log10(meanvals_jaxgrad[:,-1]), 1)[0]\n",
    "n_compl_an = np.polyfit(np.log10(n_set), np.log10(meanvals_analyt[:,-1]), 1)[0]\n",
    "print('Estimated complexity with respect to $n$ is: \\n O({:.3f}) for JAX; \\n O({:.3f} for analytical approach)'.format(n_compl_jx, n_compl_an))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Jax autograd performs worse both colu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "***\n",
    "## Problem 2.2 solution is here\n",
    "***\n",
    "_2._ (4 pts) Derive analytical expression for the gradient and hessian of the function $f$:\n",
    "\n",
    "$$ R(x) = \\frac{(Ax, x)}{(x, x)}, $$\n",
    "\n",
    "where $A$ is a symmetric real matrix. Why the gradient of this function is important in NLA you will know in the lectures later.\n",
    "***\n",
    "__SOLUTION__:\n",
    "\\\n",
    "$x \\in \\mathbb{R}^{n \\times 1}$\\\n",
    "$A \\in \\mathbb{R}^{n \\times n} $\\\n",
    "$Ax \\mathbb{R}^{n \\times 1} $\\\n",
    "$R(x) \\in \\mathbb{R}$\\\n",
    "We want to determine gradient with: \n",
    "$$ \\Big(\\frac{U}{V}\\Big)'=\\frac{U'V - UV'}{V^2}$$ \n",
    "\n",
    "$ R(x) = \\displaystyle\\frac{(Ax, x)}{(x, x)} = \\frac{(Ax)^T x}{x^T x}$\\\n",
    "\\\n",
    "$R'(x)= \\displaystyle\\frac{(x^TA^Tx)' \\cdot x^Tx - (x^TA^Tx) \\cdot (x^T x)'}{(x^T x)^2} = \\frac{(A^Tx + Ax)x^Tx - x^TA^Tx \\cdot 2x}{(x^T x)^2} =$\\\n",
    "$= \\displaystyle\\frac{A^Txx^Tx + Axx^Tx - 2(x^TA^Tx)x}{(x^T x)^2} = $\\\n",
    "$= \\displaystyle\\frac{A^Tx}{x^Tx} + \\frac{Ax}{x^Tx} - \\Big(\\frac{(x^TA^Tx)x}{(x^Tx)^2} + \\frac{(x^TAx)x}{(x^Tx)^2}\\Big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$R\"(x)= \\Bigg(\\displaystyle\\frac{A^Tx}{x^Tx} + \\frac{Ax}{x^Tx} - \\Big(\\frac{(x^TA^Tx)x}{(x^Tx)^2} + \\frac{(x^TAx)x}{(x^Tx)^2}\\Bigg)'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "***\n",
    "# Problem 3. Compression of the fully-connected layers in neural network with simple architecture (30 pts)\n",
    "\n",
    "In this problem we consider the neural network that performs classification of the dataset of images. \n",
    "Any neural network can be considered as composition of simple linear and non-linear functions.\n",
    "For example, a neural network with 3 layers can be represented as \n",
    "\n",
    "$$f_3(f_2(f_1(x, w_1), w_2), w_3),$$\n",
    "\n",
    "where $x$ is input data (in our case it will be images) and $w_i, \\; i =1,\\dots,3$ are parameters that are going to be trained. \n",
    "\n",
    "We will study the compression potential of neural network with simple architecture: alternating some numbers of linear and non-linear functions. \n",
    "\n",
    "The main task in this problem is to study how the compression of fully-connected layers affects the test accuracy.\n",
    "Any fully-connected layer is represented as linear function $AX + B$, where $X$ is input matrix and $A, B$ are trainable matrices. Matrices $A$ in every layer are going to be compressed.\n",
    "The main result that you should get is the plot of dependence of test accuracy on the total number of parameters in the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero step: install PyTorch\n",
    "\n",
    "- Follow the steps in [official instructions](https://pytorch.org/get-started/locally/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First step: download CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(datasets.CIFAR10('./', train=True, download=True, transform=transform), \n",
    "                                        batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(datasets.CIFAR10('./', train=False, transform=transform), \n",
    "                                          batch_size=batch_size, shuffle=True)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check what images are we going to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(8)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second step: neural network architecture\n",
    "\n",
    "For simplicity and demonstration purposes of the neural network compression idea consider the architecture consisting of the only fully-connected layers and non-linear ReLU functions between them.\n",
    "To demonstrate compression effect, consider the dimension of the inner layers equals to 1000.\n",
    "\n",
    "Below you see implementation of such neural network in PyTorch.\n",
    "More details about neural networks you will study in the *Deep learning* course in one of the upcoming term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, middle_features, activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_features, middle_features)\n",
    "        self.layer2 = nn.Linear(middle_features, in_features)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.layer2(out)\n",
    "        out = out + x\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(3 * 32 * 32, 1000)\n",
    "        self.res1 = ResidualBlock(1000,1000)\n",
    "        self.res2 = ResidualBlock(1000,1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "        self.ReLU = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x.view(-1, 3 * 32*32))\n",
    "        x = self.ReLU(x)\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement functions for training and testing after every sweep over all dataset entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters for training and print intermediate loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 50\n",
    "epochs = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third step: run training with the [Adam](https://arxiv.org/pdf/1412.6980.pdf%20%22%20entire%20document) optimization method\n",
    "\n",
    "If your laptop is not very fast, you will wait some time till training is finished. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model,  train_loader, optimizer, epoch)\n",
    "    test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have somehow trained neural network and we are ready to perform compression of the weigths in the fully-connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (3 pts) Compute SVD of the matrix $1000 \\times 1000$, which corresponds to a weight matrix $A$ in each layer of residual blocks. To find more information about accessing this matrix please refer to [PyTorch manual](https://pytorch.org/docs/stable/index.html).\n",
    "Plot decaying of the singular values like it was shown in the lecture. What conclusion can you make?\n",
    "- (17 pts) Create a new model, which is an analogue to the class ```Net```, but with some significant distinctions. \n",
    "It takes as input parameters the instance of the class ```Net``` and compression rank $r > 0$. \n",
    "After that, this model has to compress all matrices $A$ in fully-connected layers with SVD using first $r$ singular vectors and singular values.\n",
    "Pay attention to efficient storing of compress representation of the layers.\n",
    "Also ```forward``` method of your new residual block has to be implemented in a way to use compressed representation of the fully-connected layers (inside it) in the most efficient way. In all other aspects it has to reproduce ```forward``` method in the original non-compressed model (number of layers, activations, loss function etc).\n",
    "- (5 pts) Plot dependence of test accuracy on the number of parameters in the compressed model. This number of parameters obviously depends on the compression rank $r$.\n",
    "Also plot dependence of time to compute inference on the compression rank $r$.\n",
    "Explain obtained results.\n",
    "To measure time, use [%timeit](https://docs.python.org/3.6/library/timeit.html) with necessary parameters (examples of using this command see in lectures) \n",
    "\n",
    "- (5 pts) After such transformations, your model depends on the factors obtained from SVD. Therefore, these factors are also can be trained with the same gradient method during some number of epochs. This procedure is called **fine-tuning**. We ask you make this fine-tuning of your compressed model during from 1 to 5 epochs and compare the result test accuracy with the test accuracy that you get after compression. Explain the observed results.\n",
    "\n",
    "    **Hint 1**. Please, check that during fine-tuning, factors are updated. If they are not updated according to some reason, please check that their parameter ```requires_grad``` is setted to ```True```.\n",
    "    \n",
    "    **Hint 2**. You can use class ```torch.nn.Parameter``` to convert factors to parameters of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4. «Reducio!» (15 pts)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Hermione Granger is well versed in all magical disciplines. In particular, she is a great expert in Numerical Linear Algebra and JAX.\n",
    "\n",
    "She has invited Harry Potter to play the game. \n",
    "\n",
    "Hermione chooses a number $r \\in [1, 95]$ and two matrices $W_1 \\in \\mathbb{R}^{r \\times 100}$ and $W_2 \\in \\mathbb{R}^{100 \\times r}$.\n",
    "Harry can tell her any 100-dimensional vector $x$, and Hermione gives Harry the result of \n",
    "\n",
    "$$ ||\\sin(W_2 \\cos(W_1 x))||^2_2,$$ \n",
    "\n",
    "where trigonometric functions are applied element-wise. The result is the squared norm of a 100-dimensional vector.\n",
    "\n",
    "Not to lose, Harry should guess what number $r$ Hermione has chosen.   \n",
    "Harry knows the python language, but he is an absolute layman in algebra. Please, help him to get at least 95% correct answers! \n",
    "\n",
    "<i> Hint 1: SVD might help you, but use it wisely! </i>\n",
    "\n",
    "<i> Hint 2: Suppose that a special magic allows you to compute gradients through automatic differentiation in JAX. You can also estimate gradients via finite differences, if you want it.</i>\n",
    "\n",
    "<i> Hint 3: You can estimate the matrix rank using simple heuristics.\n",
    "    \n",
    "You should write code into the `harry_answers` function. Good luck! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from jax import jacfwd, jacrev, jit\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Game:\n",
    "    def __init__(self, key):\n",
    "        # key is a jax.random.PRNGKey\n",
    "        self.key = key\n",
    "        return\n",
    "    def hermione_chooses_r_and_W(self):\n",
    "        self.r = random.randint(1, 95)\n",
    "        \n",
    "        self.key, subkey = jax.random.split(self.key)\n",
    "        self.W1 = jax.random.uniform(subkey, \n",
    "                                     (self.r, 100), \n",
    "                                     maxval=100., \n",
    "                                     minval=0., \n",
    "                                     dtype=jnp.float32)\n",
    "        \n",
    "        self.key, subkey = jax.random.split(self.key)\n",
    "        self.W2 = jax.random.uniform(subkey, \n",
    "                                     (100, self.r), \n",
    "                                     maxval=100., \n",
    "                                     minval=0., \n",
    "                                     dtype=jnp.float32)\n",
    "        # print(str(self.r) + \"\\n\")\n",
    "    def hermione_computes_function(self, x):\n",
    "        # itsLeviOsa = jnp.sum(jnp.square(jnp.sin(self.W2 @ jnp.cos(self.W1 @ x))))\n",
    "        itsLeviOsa = jnp.sin(self.W2 @ jnp.cos(self.W1 @ x))\n",
    "        return itsLeviOsa\n",
    "    \n",
    "    def harry_answers(self):\n",
    "        x = np.random.randn(100)\n",
    "        self.y = self.hermione_computes_function(x)\n",
    "        self.grad = jacrev(self.hermione_computes_function)(x)\n",
    "        _, s, _ = np.linalg.svd(self.grad)\n",
    "        ex_pelleARMUS = s[s > 1.].shape[0] #Chosen with a magic provision\n",
    "        print(ex_pelleARMUS)\n",
    "        return ex_pelleARMUS\n",
    "    \n",
    "    def play(self, n_rounds, verbose=True):\n",
    "        # n_rounds: a number of rounds of the game\n",
    "        # verbose: print or not the result of each round\n",
    "        n_right_answers = 0\n",
    "        for _ in range(n_rounds):\n",
    "            self.hermione_chooses_r_and_W()\n",
    "            r = self.harry_answers()\n",
    "            if abs(r - self.r) == 0:\n",
    "                if verbose:\n",
    "                    print(\"Good job! The true answer is {}, Harry's answer is {}!\".format(self.r, r))\n",
    "                n_right_answers += 1\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(\"Harry's answers is {}, but the true answer is {} :(\".format(r, self.r))\n",
    "        if float(n_right_answers) / n_rounds > 0.95:\n",
    "            print('Well done: {}/{} right answers!'.format(n_right_answers, n_rounds))\n",
    "        else:\n",
    "            print('Only {}/{} right answers :( Work a little more and you will succeed!'.format(\n",
    "                n_right_answers, n_rounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(713)\n",
    "game = Game(key)\n",
    "game.play(n_rounds=100, verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
